git clone https://github.com/ggerganov/llama.cpp
pip install -r llama.cpp/requirements.txt
python3 convert.py models/7B/

  # [Optional] for models using BPE tokenizers
  python convert.py models/7B/ --vocabtype bpe
python3 llama.cpp/convert-hf-to-gguf.py /home/bordispam/Ollama-Companion/llama.cpp/models/Airoboros-c34b-2.2.1-Mistral \
  --outfile lekker.gguf \
  --outtype q8_0

  ./quantize ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0 
Q4_0
Q4_1
Q5_0
Q5_1
Q2_K
Q3_K
Q3_K_S
Q3_K_M
Q3_K_L
Q4_K
Q4_K_S
Q4_K_M
Q5_K
Q5_K_S
Q5_K_M
Q6_K
Q8_0
F16
F32


curl http://localhost:11434/api/generate -d '{
  "model": "next",
  "prompt": "Tell me more about evolution?",
  "format": "json",
  "stream": false
}'

curl http://localhost:11434/api/generate -d '{
  "model": "next",
  "prompt": "What color is the sky at different times of the day? Respond using JSON",
  "format": "json",
  "stream": false
}'