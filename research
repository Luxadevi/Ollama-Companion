git clone https://github.com/ggerganov/llama.cpp
pip install -r llama.cpp/requirements.txt
python3 convert.py models/7B/

  # [Optional] for models using BPE tokenizers
  python convert.py models/7B/ --vocabtype bpe
python llama.cpp/convert.py textareainput \
  --outfile variable \
  --outtype q8_0 f32 f16 

  ./quantize --help ./models/7B/ggml-model-f16.gguf ./models/7B/ggml-model-q4_0.gguf q4_0 
Q4_0
Q4_1
Q5_0
Q5_1
Q2_K
Q3_K
Q3_K_S
Q3_K_M
Q3_K_L
Q4_K
Q4_K_S
Q4_K_M
Q5_K
Q5_K_S
Q5_K_M
Q6_K
Q8_0
F16
F32
